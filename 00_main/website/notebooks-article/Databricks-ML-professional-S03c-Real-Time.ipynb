{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fab4bb-231e-48cf-8ed4-fc15a1b22845",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h4 style=\"font-variant-caps: small-caps;font-size:35pt;\">Databricks-ML-professional-S03c-Real-Time</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19987518-c1d2-4f7d-8679-21a874edf775",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:black;border-radius:5px;border-top:1px solid'></div>\n",
    "<br/>\n",
    "<p>This Notebook adds information related to the following requirements:</p><br/>\n",
    "<b>Real-time:</b>\n",
    "<ul>\n",
    "<li>Describe the benefits of using real-time inference for a small number of records or when fast prediction computations are needed</li>\n",
    "<li>Identify JIT feature values as a need for real-time deployment</li>\n",
    "<li>Describe model serving deploys and endpoint for every stage</li>\n",
    "<li>Identify how model serving uses one all-purpose cluster for a model deployment</li>\n",
    "<li>Query a Model Serving enabled model in the Production stage and Staging stage</li>\n",
    "<li>Identify how cloud-provided RESTful services in containers is the best solution for production-grade real-time deployments</li>\n",
    "</ul>\n",
    "<br/>\n",
    "<p><b>Download this notebook at format ipynb <a href=\"Databricks-ML-professional-S03c-Real-Time.ipynb\">here</a>.</b></p>\n",
    "<br/>\n",
    "<div style='background-color:black;border-radius:5px;border-top:1px solid'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6aaf81-c559-44bd-bc70-25852c40193d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"realtime\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">1. Describe the benefits of using real-time inference for a small number of records or\n",
    "when fast prediction computations are needed</span></div>\n",
    "<ul>\n",
    "<li>For on-demand response</li>\n",
    "<li>Generates predictions for a small number of records with fast results (e.g. results in milliseconds)</li>\n",
    "<li>Rely on REST API development - need to create a REST endpoint for example MLflow model serving endpoint</li>\n",
    "<li>Real-time or near Real-time predictions</li>\n",
    "<li>Has lowest latency but also highest costs because it requires serving infrastructures which have a cost</li>\n",
    "<li>Users provide data to the model through REST API, model predicts the target in real-time</li>\n",
    "<li>5-10% of use cases</li>\n",
    "<li>Example of use cases: Financial (fraud detection), mobile, ad tech</li></ul>\n",
    "<div style=\"display:block;text-align:center\"><img width=\"500px\" src=\"https://i.ibb.co/rxzz2vS/databricks-ml-pro-latency.png\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e681ce-93ed-4c38-814e-6d851bb56281",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">2. Identify JIT feature values as a need for real-time deployment</span></div>\n",
    "<p>N/A</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d425d7d-6963-4712-8b0b-f082aa43e8ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"modelservingendpoints\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">3. Describe model serving deploys and endpoint for every stage</span></div>\n",
    "<p><i>You can use a serving endpoint to serve models from the Databricks Model Registry or from Unity Catalog.</i></p><p><i>Endpoints expose the underlying models as scalable REST API endpoints using serverless compute. This means the endpoints and associated compute resources are fully managed by Databricks and will not appear in your cloud account.</i></p><p><i>A serving endpoint can consist of one or more MLflow models from the Databricks Model Registry, called served models.</i></p><p><i>A serving endpoint can have at most ten served models.</i></p><p><i>You can configure traffic settings to define how requests should be routed to your served models behind an endpoint.</i></p><p><i>Additionally, you can configure the scale of resources that should be applied to each served model.</i></p><p><a href=\"https://docs.databricks.com/api/workspace/servingendpoints\" target=\"_blank\">source</a></p>\n",
    "<p>For more information about how to create a model serving enpoint using MLflow, see <a href=\"https://customer-academy.databricks.com/learn/course/1522/play/9706/real-time-demo\" target=\"_blank\">this video</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af303554-0625-435f-96e3-3aa2b0e71983",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<ul>\n",
    "<li>A model need to be <b>logged</b> and <b>registered</b> to MLflow before being linked to a serving endpoint</li>\n",
    "</ul>\n",
    "<i>see previous chapters and/or <a href=\"https://customer-academy.databricks.com/learn/course/1522/play/9706/real-time-demo\" target=\"_blank\">this video</a>.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042bc138-2b20-4ecb-b498-edd5219e4b6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<ul>\n",
    "<li>Model(s) to be served should be selected at endpoint creation by the selection of model(s) name and model(s) version</li>\n",
    "<img width=\"1000px\" src=\"https://i.ibb.co/vdgL1mD/servingendpointcreation1.png\"/>\n",
    "<li><b>Up to 10 models can be served</b> and <b>percentage of traffic</b> for each of them is <b>configurable</b>:</li>\n",
    "<img width=\"1000px\" src=\"https://i.ibb.co/1ngQgKP/multiplemodels.png\"/>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ab5554e-4be0-428c-9e54-33a8ac5dfed2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<ul>\n",
    "<li>A newly created endoint is disabled. It will become active after having been enabled.</li>\n",
    "</ul>\n",
    "<b>Here after are the lines to enable a serving enpoint:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ccf34e-d541-4cc9-a1b1-a88b82733c8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import requests\n",
    "#\n",
    "# this is to get a temporary token. Best is to create a token within Databricks interface\n",
    "token = mlflow.utils.databricks_utils._get_command_context().apiToken().get()\n",
    "#\n",
    "# With the token, create the authorization header for the subsequent REST calls\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "#\n",
    "# get endpoint at which to execute the request\n",
    "api_url = mlflow.utils.databricks_utils.get_webapp_url()\n",
    "#\n",
    "# create the url\n",
    "url = f\"{api_url}/api/2.0/mlflow/endpoints/enable\"\n",
    "#\n",
    "# send request to enable endpoint\n",
    "requests.post(url, headers=headers, json={\"registered_model_name\": \"<model_name>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "696921a2-200a-440c-a54b-b91705b029bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<ul>\n",
    "<li>User who need to create a model serving endpoint in MLflow will need <b>cluster creation persmission</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fe25cac-1bde-40e0-91ed-609ff3a34162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"allpurpose\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">4. Identify how model serving uses one all-purpose cluster for a model deployment</span></div>\n",
    "<p>The purpose of a served model is to provide predictions in real-time. When users or anyone/any service make a request to the endpoint to get predictions, he/it should not have to wait for a cluster to start, results should be provided instantly. Serving endpoints use serverless compute. See <a href=\"https://learn.microsoft.com/en-us/azure/databricks/serverless-compute/\" tager=\"_blank\">this page</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78054f4d-c49b-4b63-a8d5-7ee9e8d64c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"querymodelserving\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">5. Query a Model Serving enabled model in the Production stage and Staging stage</span></div>\n",
    "<p>Hereafter is the minimal Python code to use to get predictions from a served model. Model can be either in Production stage or Staging stage, the way to get predictions is the same.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "395a7524-a272-42e4-9920-7f685de5c714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this is to get a temporary token. Best is to create a token within Databricks interface\n",
    "token = mlflow.utils.databricks_utils._get_command_context().apiToken().get()\n",
    "#\n",
    "# With the token, create the authorization header for the subsequent REST calls\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "#\n",
    "# get endpoint at which to execute the request\n",
    "api_url = mlflow.utils.databricks_utils.get_webapp_url()\n",
    "#\n",
    "# create url\n",
    "url = f\"{api_url}/model/<model_name>/invocations\"\n",
    "#\n",
    "# data to predict should be formatted this way. As an example, let's consider we want to predict X_test\n",
    "ds_dict = X_test.to_dict(orient=\"split\")\n",
    "#\n",
    "# request predictions\n",
    "response = requests.request(method=\"POST\", headers=headers, url=url, json=ds_dict)\n",
    "#\n",
    "# for predictions in JSON, this is the command\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3d086e7-6d9f-41eb-84ae-b972f71263ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<p>Alternatively, <b>sample url or code (Curl/Python)</b> to make a request and get predictions from a served model is provided in the Serving UI <i>(source: <a href=\"https://customer-academy.databricks.com/learn/course/1522/play/9706/real-time-demo\" target=\"_blank\">this video</a>)</i>:</p>\n",
    "<img width=\"1000px\" src=\"https://i.ibb.co/KrQtDNZ/serving.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2670c1-d060-4cde-8764-97e036c7790d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"cloudprovidedrestfulservices\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">6. Identify how cloud-provided RESTful services in containers is the best solution for\n",
    "production-grade real-time deployments</span></div>\n",
    "<p>Containers are suitable for real-time production deployments due to their ease of management, lightweight characteristics, and scalable capabilities facilitated by services like Kubernetes.</p>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1158789969180638,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Databricks-ML-professional-S03c-Real-Time",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
