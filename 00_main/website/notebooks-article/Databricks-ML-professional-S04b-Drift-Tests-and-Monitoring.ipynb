{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fab4bb-231e-48cf-8ed4-fc15a1b22845",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h4 style=\"font-variant-caps: small-caps;font-size:35pt;\">Databricks-ML-professional-S04b-Drift-Tests-and-Monitoring</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70d28f6a-8dc1-4a86-952f-ebf6ac82479b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style='background-color:black;border-radius:5px;border-top:1px solid'></div>\n",
    "<br/>\n",
    "<p>This Notebook adds information related to the following requirements:</p><br/>\n",
    "<b>Drift Tests and Monitoring:</b>\n",
    "<ul>\n",
    "<li>Describe summary statistic monitoring as a simple solution for numeric feature drift</li>\n",
    "<li>Describe mode, unique values, and missing values as simple solutions for categorical feature drift</li>\n",
    "<li>Describe tests as more robust monitoring solutions for numeric feature drift than simple summary statistics</li>\n",
    "<li>Describe tests as more robust monitoring solutions for categorical feature drift than simple summary statistics</li>\n",
    "<li>Compare and contrast Jenson-Shannon divergence and Kolmogorov-Smirnov tests for numerical drift detection</li>\n",
    "<li>Identify a scenario in which a chi-square test would be useful</li>\n",
    "</ul>\n",
    "<br/>\n",
    "<p><b>Download this notebook at format ipynb <a href=\"Databricks-ML-professional-S04b-Drift-Tests-and-Monitoring.ipynb\">here</a>.</b></p>\n",
    "<br/>\n",
    "<div style='background-color:black;border-radius:5px;border-top:1px solid'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6aaf81-c559-44bd-bc70-25852c40193d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"numericalfeaturedrift\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">1. Describe summary statistic monitoring as a simple solution for numeric feature drift</span></div><p>Summary statistic monitoring is a straightforward approach to detect numeric feature drift. The idea is to calculate summary statistics (such as mean, standard deviation, minimum, maximum, etc.) for each numeric feature in the training data and then compare these statistics with the summary statistics of the incoming data in the production environment. Deviations from the expected summary statistics can indicate feature drift.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e681ce-93ed-4c38-814e-6d851bb56281",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"categoricalfeaturedrift\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">2. Describe mode, unique values, and missing values as simple solutions for categorical\n",
    "feature drift</span></div>\n",
    "<ul><li><b>Mode Monitoring</b>:</li>\n",
    "<ul><li><b>\n",
    "Definition</b>: The mode of a categorical feature is the value that appears most frequently.</li>\n",
    "<li><b>\n",
    "Implementation</b>:\n",
    "During the training phase, identify the mode of each categorical feature in the training dataset. In the production environment, monitor the mode of each categorical feature in the incoming data. If the mode shifts significantly, it could indicate a change in the distribution of categories, suggesting possible drift.\n",
    "</li></ul>\n",
    "<li><b>Unique Values Monitoring</b>:</li>\n",
    "<ul><li><b>\n",
    "Definition</b>: The set of unique values in a categorical feature represents the different categories present.</li><li><b>\n",
    "Implementation</b>:\n",
    "Calculate the unique values of each categorical feature in the training data. Continuously monitor the unique values of each categorical feature in the production data. If new, unexpected categories appear or if existing categories disappear, it may indicate drift.</li></ul>\n",
    "<li><b>\n",
    "Missing Values Monitoring</b>:</li>\n",
    "<ul><li><b>\n",
    "Definition</b>: Changes in the frequency of missing values can also indicate drift.</li><li><b>\n",
    "Implementation</b>:\n",
    "Record the percentage of missing values for each categorical feature during training.\n",
    "Monitor the percentage of missing values for each categorical feature in the production data.\n",
    "Drift may be present if the rate of missing values changes significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d425d7d-6963-4712-8b0b-f082aa43e8ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"testsmorerobustfornumericfeatures\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">3. Describe tests as more robust monitoring solutions for numeric feature drift than simple summary statistics</span></div>\n",
    "<p>Tests offer more robust solutions for monitoring numeric feature drift than simple summary statistics. Instead of relying solely on mean or standard deviation, statistical tests provide a formalized way to assess the significance of differences in feature distributions. For instance, the Kolmogorov-Smirnov test or the Anderson-Darling test can compare the cumulative distribution functions of training and production data. These tests consider the entire distribution, making them sensitive to subtle shifts. Additionally, the Cram√©r-von Mises test can evaluate differences in distribution shapes, offering a more nuanced analysis. Implementing these tests allows for a systematic and statistical approach to detect numeric feature drift, enhancing the model's adaptability to evolving data patterns in a production environment.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5bc2486-22b4-4463-a8f7-9cacc347db73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"testsmorerobustforcatfeatures\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">4. Describe tests as more robust monitoring solutions for categorical feature drift than simple summary statistics</span></div>\n",
    "<p>Tests provide robust solutions for monitoring categorical feature drift compared to simple summary statistics. Rather than relying solely on mode or unique values, statistical tests offer a more formalized approach. For instance, the chi-squared test assesses the independence of observed and expected categorical distributions, indicating if there are significant deviations. This test is particularly valuable when dealing with multiple categories. Another option is the G-test, which is an extension of the chi-squared test and is suitable for smaller sample sizes. By employing these statistical tests, it becomes possible to systematically identify shifts in categorical feature distributions between training and production data, allowing for a more nuanced and reliable detection of drift in real-world scenarios.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f958033b-36f3-41c4-b525-7eeb4e168a42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"compareJSDandKS\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">5. Compare and contrast Jenson-Shannon divergence and Kolmogorov-Smirnov tests for numerical drift detection</span></div>\n",
    "<p>The Jenson-Shannon Divergence (JSD) and Kolmogorov-Smirnov (KS) test are both methods for detecting numerical drift, but they operate on different principles.</p>\n",
    "<p><b>Jenson-Shannon Divergence (JSD)</b>:\n",
    "JSD measures the similarity between two probability distributions by computing the divergence between their probability mass functions. In drift detection, JSD can quantify the difference in probability distributions of numeric features between training and production data. It considers the entire distribution, providing a comprehensive analysis. However, it requires a smooth distribution and may be sensitive to outliers.</p>\n",
    "<p>\n",
    "<i>Jensen Shannon (JS) distance is more appropriate for drift detection on a large dataset since it meaures the distance between two probability distributions and it is smoothed and normalized. When log base 2 is used for the distance calculation, the JS statistic is bounded between 0 and 1:</i>\n",
    "\n",
    "<ul><li><i>0 means the distributions are identical</i></li>\n",
    "<li><i>1 means the distributions have no similarity</i></li></ul>\n",
    "</p>\n",
    "\n",
    "<p><b>Kolmogorov-Smirnov Test (KS)</b>:\n",
    "KS test assesses the similarity of two cumulative distribution functions (CDFs) and is sensitive to differences anywhere in the distribution. It calculates the maximum vertical distance between the CDFs, providing a simple and non-parametric measure. KS is less affected by outliers but might be influenced by sample size.</p><p><i>This test determines whether or not two different samples come from the same distribution.</i></p>\n",
    "<ul>\n",
    "    <li><i>Returns a higher KS statistic when there is a higher probability of having two different distributions</i></li>\n",
    "    <li><i>Returns a lower P value the higher the statistical significance</i></li>\n",
    "    <li><i>In practice, we need a threshold for the p-value, where we will consider it <b>unlikely enough</b> that the samples did not come from the same distribution. Usually this threshold, or alpha level, is 0.05.</i></li></ul>\n",
    "\n",
    "<p>In summary, JSD is more comprehensive and suitable for smooth distributions, while KS is robust, especially against outliers, but its sensitivity to sample size should be considered. The choice depends on the specific characteristics of the data and the desired balance between sensitivity and robustness.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78c6589c-d9e1-4dcd-b564-0fa83dd87d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<a id=\"chisquarescenario\"></a>\n",
    "<div style='background-color:rgba(30, 144, 255, 0.1);border-radius:5px;padding:2px;'>\n",
    "<span style=\"font-variant-caps: small-caps;font-weight:700\">6. Identify a scenario in which a chi-square test would be useful</span></div>\n",
    "<p>A chi-square test would be useful in scenarios involving categorical data and the need to assess the independence or association between two categorical variables. One prominent example is in medical research when investigating the relationship between smoking status (categories: smoker, non-smoker) and the incidence of a specific health outcome (categories: presence, absence).</p><p>\n",
    "\n",
    "Consider a clinical study aiming to understand whether there is a significant association between smoking habits and the development of a particular respiratory condition. Researchers collect data on a sample of individuals, categorizing them based on smoking status and the presence or absence of the respiratory condition. By applying the chi-square test, they can analyze the observed and expected frequencies in a contingency table, determining whether any observed associations are statistically significant or if they could have occurred by chance.</p><p>\n",
    "\n",
    "The chi-square test provides a valuable statistical tool in such scenarios, helping researchers draw conclusions about the independence of variables and contributing insights into potential causal relationships, ultimately informing public health strategies or medical interventions.</p>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1158789969180638,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Databricks-ML-professional-S04b-Drift-Tests-and-Monitoring",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
